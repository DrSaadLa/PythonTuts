{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font size=6, color=\"#7B242F\"><u>Gradient Boosting Machine: The Foundation of Extreme Gradient Boosting</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Concepts\n",
    "\n",
    "\n",
    "### Boosting Overview\n",
    "\n",
    " - $\\textbf{Boosting}$ is a concept that can be applied to a set of machine learning models rather than a specific machine learning algorithm per se. It is an $\\textbf{ensemble meta-algorithm}$ that helps to improve model performance and accuracy by taking a group of weak learners and combining them to form a strong learner.\n",
    "\n",
    "- The idea behind boosting is that predictors should learn from mistakes that have been made by previous predictors.  \n",
    "\n",
    "- There are many boosting algorithms such as Adaboost, gradient boosting ...etc.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "- Gradient Boosting algorithm works by sequentially adding predictors to an ensemble, each one correcting its predecessor. This method tries to fit the new predictor to the residual errors made by the previous predictor. It has two key characteristics:\n",
    "    - Undergoes multiple iterations.\n",
    "    - Each iteration focuses on the instances that were wrongly classified by previous iterations\n",
    "\n",
    "For more information about GBM check this [notebook](https://github.com/DrSaadLa/PythonTuts/blob/main/TreeBasedModels/06.%2001.%20Gradient%20Boosting%20Machine.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font size=7, color=\"#7B242F\"><u>Hands on Rental Bikes Project</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size=6, color=\"#990099\">1. Rental Bikes Exploratory Data Analysis </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================================================================\n",
    "#            Importing the necessary modules and tools\n",
    "## ======================================================================\n",
    "\n",
    "import pandas as pd; import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Import XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# from sklearn.metrics import necessary metrics\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Preprocessing tools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Set notebook options\n",
    "# --------------------\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "%matplotlib inline\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?GradientBoostingRegressor (for help about the regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     731 non-null    int64  \n",
      " 1   dteday      731 non-null    object \n",
      " 2   season      731 non-null    float64\n",
      " 3   yr          730 non-null    float64\n",
      " 4   mnth        730 non-null    float64\n",
      " 5   holiday     731 non-null    float64\n",
      " 6   weekday     731 non-null    float64\n",
      " 7   workingday  731 non-null    float64\n",
      " 8   weathersit  731 non-null    int64  \n",
      " 9   temp        730 non-null    float64\n",
      " 10  atemp       730 non-null    float64\n",
      " 11  hum         728 non-null    float64\n",
      " 12  windspeed   726 non-null    float64\n",
      " 13  casual      731 non-null    int64  \n",
      " 14  registered  731 non-null    int64  \n",
      " 15  cnt         731 non-null    int64  \n",
      "dtypes: float64(10), int64(5), object(1)\n",
      "memory usage: 91.5+ KB\n",
      "**********************************************************************\n",
      "   instant      dteday  season    yr  mnth  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01   1.000 0.000 1.000    0.000    6.000       0.000   \n",
      "1        2  2011-01-02   1.000 0.000 1.000    0.000    0.000       0.000   \n",
      "2        3  2011-01-03   1.000 0.000 1.000    0.000    1.000       1.000   \n",
      "3        4  2011-01-04   1.000 0.000 1.000    0.000    2.000       1.000   \n",
      "4        5  2011-01-05   1.000 0.000 1.000    0.000    3.000       1.000   \n",
      "\n",
      "   weathersit  temp  atemp   hum  windspeed  casual  registered   cnt  \n",
      "0           2 0.344  0.364 0.806      0.160     331         654   985  \n",
      "1           2 0.363  0.354 0.696      0.249     131         670   801  \n",
      "2           1 0.196  0.189 0.437      0.248     120        1229  1349  \n",
      "3           1 0.200  0.212 0.590      0.160     108        1454  1562  \n",
      "4           1 0.227  0.229 0.437      0.187      82        1518  1600  \n",
      "**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "## ================================================\n",
    "#    Read and explore the data\n",
    "# =================================================\n",
    "bikes = pd.read_csv('bike_rentals.csv')\n",
    "print(\"*\"*70)\n",
    "bikes.info()\n",
    "print(\"*\"*70)\n",
    "print(bikes.head())\n",
    "print(\"*\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>instant</th>\n",
       "      <td>731.000</td>\n",
       "      <td>366.000</td>\n",
       "      <td>211.166</td>\n",
       "      <td>1.000</td>\n",
       "      <td>183.500</td>\n",
       "      <td>366.000</td>\n",
       "      <td>548.500</td>\n",
       "      <td>731.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>731.000</td>\n",
       "      <td>2.497</td>\n",
       "      <td>1.111</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr</th>\n",
       "      <td>730.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth</th>\n",
       "      <td>730.000</td>\n",
       "      <td>6.512</td>\n",
       "      <td>3.448</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>9.750</td>\n",
       "      <td>12.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>holiday</th>\n",
       "      <td>731.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>731.000</td>\n",
       "      <td>2.997</td>\n",
       "      <td>2.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>6.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workingday</th>\n",
       "      <td>731.000</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit</th>\n",
       "      <td>731.000</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.545</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>730.000</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atemp</th>\n",
       "      <td>730.000</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hum</th>\n",
       "      <td>728.000</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windspeed</th>\n",
       "      <td>726.000</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>casual</th>\n",
       "      <td>731.000</td>\n",
       "      <td>848.176</td>\n",
       "      <td>686.622</td>\n",
       "      <td>2.000</td>\n",
       "      <td>315.500</td>\n",
       "      <td>713.000</td>\n",
       "      <td>1,096.000</td>\n",
       "      <td>3,410.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>registered</th>\n",
       "      <td>731.000</td>\n",
       "      <td>3,656.172</td>\n",
       "      <td>1,560.256</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2,497.000</td>\n",
       "      <td>3,662.000</td>\n",
       "      <td>4,776.500</td>\n",
       "      <td>6,946.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnt</th>\n",
       "      <td>731.000</td>\n",
       "      <td>4,504.349</td>\n",
       "      <td>1,937.211</td>\n",
       "      <td>22.000</td>\n",
       "      <td>3,152.000</td>\n",
       "      <td>4,548.000</td>\n",
       "      <td>5,956.000</td>\n",
       "      <td>8,714.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count      mean       std    min       25%       50%       75%  \\\n",
       "instant    731.000   366.000   211.166  1.000   183.500   366.000   548.500   \n",
       "season     731.000     2.497     1.111  1.000     2.000     3.000     3.000   \n",
       "yr         730.000     0.500     0.500  0.000     0.000     0.500     1.000   \n",
       "mnth       730.000     6.512     3.448  1.000     4.000     7.000     9.750   \n",
       "holiday    731.000     0.029     0.167  0.000     0.000     0.000     0.000   \n",
       "weekday    731.000     2.997     2.005  0.000     1.000     3.000     5.000   \n",
       "workingday 731.000     0.683     0.466  0.000     0.000     1.000     1.000   \n",
       "weathersit 731.000     1.395     0.545  1.000     1.000     1.000     2.000   \n",
       "temp       730.000     0.496     0.183  0.059     0.337     0.499     0.656   \n",
       "atemp      730.000     0.475     0.163  0.079     0.338     0.487     0.609   \n",
       "hum        728.000     0.628     0.142  0.000     0.522     0.627     0.730   \n",
       "windspeed  726.000     0.190     0.078  0.022     0.134     0.181     0.233   \n",
       "casual     731.000   848.176   686.622  2.000   315.500   713.000 1,096.000   \n",
       "registered 731.000 3,656.172 1,560.256 20.000 2,497.000 3,662.000 4,776.500   \n",
       "cnt        731.000 4,504.349 1,937.211 22.000 3,152.000 4,548.000 5,956.000   \n",
       "\n",
       "                 max  \n",
       "instant      731.000  \n",
       "season         4.000  \n",
       "yr             1.000  \n",
       "mnth          12.000  \n",
       "holiday        1.000  \n",
       "weekday        6.000  \n",
       "workingday     1.000  \n",
       "weathersit     3.000  \n",
       "temp           0.862  \n",
       "atemp          0.841  \n",
       "hum            0.973  \n",
       "windspeed      0.507  \n",
       "casual     3,410.000  \n",
       "registered 6,946.000  \n",
       "cnt        8,714.000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic Descriptive statistics\n",
    "# ----------------------------\n",
    "bikes.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instant       0\n",
       "dteday        0\n",
       "season        0\n",
       "yr            1\n",
       "mnth          1\n",
       "holiday       0\n",
       "weekday       0\n",
       "workingday    0\n",
       "weathersit    0\n",
       "temp          1\n",
       "atemp         1\n",
       "hum           3\n",
       "windspeed     5\n",
       "casual        0\n",
       "registered    0\n",
       "cnt           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing data\n",
    "# ------------------------\n",
    "bikes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>2011-02-26</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>424</td>\n",
       "      <td>1545</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>2011-03-23</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>1918</td>\n",
       "      <td>2121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>129</td>\n",
       "      <td>2011-05-09</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>664</td>\n",
       "      <td>3698</td>\n",
       "      <td>4362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>130</td>\n",
       "      <td>2011-05-10</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116</td>\n",
       "      <td>694</td>\n",
       "      <td>4109</td>\n",
       "      <td>4803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>214</td>\n",
       "      <td>2011-08-02</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.206</td>\n",
       "      <td>801</td>\n",
       "      <td>4044</td>\n",
       "      <td>4845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>299</td>\n",
       "      <td>2011-10-26</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>404</td>\n",
       "      <td>3490</td>\n",
       "      <td>3894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>389</td>\n",
       "      <td>2012-01-24</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.124</td>\n",
       "      <td>439</td>\n",
       "      <td>3900</td>\n",
       "      <td>4339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>529</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>477</td>\n",
       "      <td>4495</td>\n",
       "      <td>4972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>702</td>\n",
       "      <td>2012-12-02</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.124</td>\n",
       "      <td>892</td>\n",
       "      <td>3757</td>\n",
       "      <td>4649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>731</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.155</td>\n",
       "      <td>439</td>\n",
       "      <td>2290</td>\n",
       "      <td>2729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     instant      dteday  season    yr   mnth  holiday  weekday  workingday  \\\n",
       "56        57  2011-02-26   1.000 0.000  2.000    0.000    6.000       0.000   \n",
       "81        82  2011-03-23   2.000 0.000  3.000    0.000    3.000       1.000   \n",
       "128      129  2011-05-09   2.000 0.000  5.000    0.000    1.000       1.000   \n",
       "129      130  2011-05-10   2.000 0.000  5.000    0.000    2.000       1.000   \n",
       "213      214  2011-08-02   3.000 0.000  8.000    0.000    2.000       1.000   \n",
       "298      299  2011-10-26   4.000 0.000 10.000    0.000    3.000       1.000   \n",
       "388      389  2012-01-24   1.000 1.000  1.000    0.000    2.000       1.000   \n",
       "528      529  2012-06-12   2.000 1.000  6.000    0.000    2.000       1.000   \n",
       "701      702  2012-12-02   4.000 1.000 12.000    0.000    0.000       0.000   \n",
       "730      731  2012-12-31   1.000   NaN    NaN    0.000    1.000       0.000   \n",
       "\n",
       "     weathersit  temp  atemp   hum  windspeed  casual  registered   cnt  \n",
       "56            1 0.282  0.282 0.538        NaN     424        1545  1969  \n",
       "81            2 0.347  0.338 0.840        NaN     203        1918  2121  \n",
       "128           1 0.532  0.525 0.589        NaN     664        3698  4362  \n",
       "129           1 0.532  0.523   NaN      0.116     694        4109  4803  \n",
       "213           1 0.783  0.707   NaN      0.206     801        4044  4845  \n",
       "298           2 0.484  0.473 0.720        NaN     404        3490  3894  \n",
       "388           1 0.343  0.349   NaN      0.124     439        3900  4339  \n",
       "528           2 0.653  0.598 0.833        NaN     477        4495  4972  \n",
       "701           2   NaN    NaN 0.823      0.124     892        3757  4649  \n",
       "730           2 0.216  0.223 0.578      0.155     439        2290  2729  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the null values in the data set\n",
    "# ------------------------------------\n",
    "bikes[bikes.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56    NaN\n",
       "81    NaN\n",
       "128   NaN\n",
       "298   NaN\n",
       "528   NaN\n",
       "Name: windspeed, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the null values in the data set\n",
    "# ------------------------------------\n",
    "bikes.loc[bikes.windspeed.isnull(), 'windspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill windspeed null values with median\n",
    "# ---------------------------------------\n",
    "\n",
    "bikes['windspeed'].fillna((bikes['windspeed'].median()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     holiday  weekday  workingday  weathersit  temp  atemp   hum  windspeed\n",
       "56     0.000    6.000       0.000           1 0.282  0.282 0.538      0.181\n",
       "81     0.000    3.000       1.000           2 0.347  0.338 0.840      0.181\n",
       "128    0.000    1.000       1.000           1 0.532  0.525 0.589      0.181\n",
       "298    0.000    3.000       1.000           2 0.484  0.473 0.720      0.181\n",
       "528    0.000    2.000       1.000           2 0.653  0.598 0.833      0.181"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display rows of windspeed that were NaNs\n",
    "# ---------------------------------------\n",
    "bikes.iloc[[56, 81, 128, 298, 528], 5:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>366.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.203</td>\n",
       "      <td>218.000</td>\n",
       "      <td>1,867.000</td>\n",
       "      <td>2,209.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.000</th>\n",
       "      <td>308.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.192</td>\n",
       "      <td>867.000</td>\n",
       "      <td>3,844.000</td>\n",
       "      <td>4,941.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.000</th>\n",
       "      <td>401.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1,050.500</td>\n",
       "      <td>4,110.500</td>\n",
       "      <td>5,353.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.000</th>\n",
       "      <td>493.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>11.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.168</td>\n",
       "      <td>544.500</td>\n",
       "      <td>3,815.000</td>\n",
       "      <td>4,634.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        instant    yr   mnth  holiday  weekday  workingday  weathersit  temp  \\\n",
       "season                                                                         \n",
       "1.000   366.000 0.500  2.000    0.000    3.000       1.000       1.000 0.286   \n",
       "2.000   308.500 0.500  5.000    0.000    3.000       1.000       1.000 0.562   \n",
       "3.000   401.500 0.500  8.000    0.000    3.000       1.000       1.000 0.715   \n",
       "4.000   493.000 0.500 11.000    0.000    3.000       1.000       1.000 0.410   \n",
       "\n",
       "        atemp   hum  windspeed    casual  registered       cnt  \n",
       "season                                                          \n",
       "1.000   0.283 0.544      0.203   218.000   1,867.000 2,209.000  \n",
       "2.000   0.538 0.647      0.192   867.000   3,844.000 4,941.500  \n",
       "3.000   0.657 0.636      0.165 1,050.500   4,110.500 5,353.500  \n",
       "4.000   0.410 0.661      0.168   544.500   3,815.000 4,634.500  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Groupby season with median\n",
    "# --------------------------\n",
    "bikes.groupby(['season']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'hum' (Humidity) null values to median of season\n",
    "# ---------------------------------------------------------\n",
    "bikes['hum'] = bikes['hum'].fillna(bikes.groupby('season')['hum'].transform('median'))\n",
    "\n",
    "# ==================================================\n",
    "#        Code disassembling\n",
    "# ==================================================\n",
    "# This code can broken into pieces if you are having\n",
    "# dificulty in understanding it. \n",
    "# 1. We replace the null values using .fillna() method\n",
    "# 2. inside the .fillna() is the value to fill with\n",
    "# 3. We want to fill with the median of each season:\n",
    "# 3.1. Using groupby() to group data by season\n",
    "# 3.2. The result of groupby needs a transform() function\n",
    "# 3.3. The transformation needs to be applied into the 'hum' column\n",
    "# We could've written the code this way\n",
    "# Step 01\n",
    "# hum_median_season = bikes.groupby('season')['hum'].transform('median')\n",
    "# Step 02\n",
    "# bikes['hum'] = bikes['hum'].fillna(hum_median_season)\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>702</td>\n",
       "      <td>2012-12-02</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.124</td>\n",
       "      <td>892</td>\n",
       "      <td>3757</td>\n",
       "      <td>4649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     instant      dteday  season    yr   mnth  holiday  weekday  workingday  \\\n",
       "701      702  2012-12-02   4.000 1.000 12.000    0.000    0.000       0.000   \n",
       "\n",
       "     weathersit  temp  atemp   hum  windspeed  casual  registered   cnt  \n",
       "701           2   NaN    NaN 0.823      0.124     892        3757  4649  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show null values of 'temp' column\n",
    "# --------------------------------\n",
    "bikes[bikes['temp'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean temp and atemp by row\n",
    "# ----------------------------------\n",
    "mean_temp = (bikes.iloc[700]['temp'] + bikes.iloc[702]['temp'])/2\n",
    "mean_atemp = (bikes.iloc[700]['atemp'] + bikes.iloc[702]['atemp'])/2\n",
    "\n",
    "# Replace null values with mean temperatures\n",
    "# ------------------------------------------\n",
    "bikes['temp'].fillna((mean_temp), inplace=True)\n",
    "bikes['atemp'].fillna((mean_atemp), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'dteday' to datetime object\n",
    "# -----------------------------------\n",
    "bikes['dteday'] = pd.to_datetime(bikes['dteday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2011-01-01\n",
       "1     2011-01-02\n",
       "2     2011-01-03\n",
       "3     2011-01-04\n",
       "4     2011-01-05\n",
       "         ...    \n",
       "726   2012-12-27\n",
       "727   2012-12-28\n",
       "728   2012-12-29\n",
       "729   2012-12-30\n",
       "730   2012-12-31\n",
       "Name: dteday, Length: 731, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes['dteday'].apply(pd.to_datetime, infer_datetime_format=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datetime\n",
    "# ----------------\n",
    "import datetime as dt\n",
    "\n",
    "# create a month column\n",
    "# ---------------------\n",
    "\n",
    "bikes['mnth'] = bikes['dteday'].dt.month\n",
    "\n",
    "# Show last 5 rows\n",
    "# ----------------\n",
    "bikes.tail()\n",
    "\n",
    "# Change row 730, column 'yr' to 1.0\n",
    "# -------------------------------\n",
    "bikes.loc[730, 'yr'] = 1.0\n",
    "\n",
    "# Drop 'dteday' column\n",
    "# --------------------\n",
    "bikes = bikes.drop('dteday', axis=1)\n",
    "\n",
    "# Drop 'casual', 'registered' columns\n",
    "# -----------------------------------\n",
    "bikes = bikes.drop(['casual', 'registered'], axis=1)\n",
    "\n",
    "# Export the data for a later use\n",
    "# ----------------------------------\n",
    "#bikes.to_csv('bike_rentals_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size=6, color=\"#990099\"> 2. Applying Machine Learning Algoritms on Rental Bikes dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X and y\n",
    "# -----------------------\n",
    "X = bikes.iloc[:,:-1]\n",
    "y = bikes.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "# -----------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Linear Regression\n",
    "# ------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split data into train and test sets\n",
    "# ------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Regression Trees Model</u> </font>\n",
    "  - The first algorithm we use to train our model is linear regression. Then we can see how can we beat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 858.8902\n"
     ]
    }
   ],
   "source": [
    "# Initialize LinearRegression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Fit lin_reg on training data\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict X_test using lin_reg\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "\n",
    "# Compute mean_squared_error as mse\n",
    "# --------------------------------\n",
    "mse = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute root mean squared error as rmse\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Display root mean squared error\n",
    "print(\"RMSE: {:0.4f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     731.000\n",
       "mean    4,504.349\n",
       "std     1,937.211\n",
       "min        22.000\n",
       "25%     3,152.000\n",
       "50%     4,548.000\n",
       "75%     5,956.000\n",
       "max     8,714.000\n",
       "Name: cnt, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display bike rental stats\n",
    "#-------------------------\n",
    "bikes['cnt'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <center><font size=6, color=\"#7B249F\"><u>  Baseline Machine Learning Algorithm: Linear Regression </u> </font>\n",
    " - We use another algorithms to build the model, and see how it performs on this data. `Regression trees`. I'll set some parameters such as `max_depth` to 2 and `random_state` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regressor if you haven't done that yet\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1128.3856\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression Tree Regressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, \n",
    "                                  random_state=1)\n",
    "\n",
    "# Fit tree_reg on training data\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict X_test using lin_reg\n",
    "y_pred = tree_reg.predict(X_test)\n",
    "\n",
    "\n",
    "# Compute mean_squared_error as mse\n",
    "# --------------------------------\n",
    "mse = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute root mean squared error as rmse\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Display root mean squared error\n",
    "print(\"RMSE: {:0.4f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It seems this algorithm performs poorly on this data because we didn't make the regressor flexible enough to capture all the patterns. However, we can't judge it fully without doing some acrobatics fine tuning this algorithm. We don't do that here but you can try to do that yourself.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Building Gradient Boosting Machine From Scratch (Manually)</u> </font>\n",
    "    \n",
    "  - We give enough of attention to this algorithm here because it is at the core of XGBoost algorithm. The hyperparameters of this gradient boosting is fully incorporated in XGBoost.\n",
    "    \n",
    "    \n",
    " - Recall: gradient boosting takes these steps to train:\n",
    "    - Fits the first tree on the training data\n",
    "    - Calculates the errors\n",
    "    - Train the second tree based on the errors of the first tree's predictions. \n",
    "    - Train the third tree on the errors of the second tree's predictions.\n",
    "    - The process is repeated until the algorithm stopped improving.\n",
    "    \n",
    "Easier said than done. huh! \n",
    "    \n",
    "We are not going to stop here, we will walk you through each step to building a Gradient boosting model. We will also focus on hyperparameter tuning. Understanding this algorithm will help you move to XGboost (which is more powerful than GBM itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(max_depth=2, random_state=1)\n"
     ]
    }
   ],
   "source": [
    "#  ============================================\n",
    "#.              Phase One\n",
    "#  ===========================================\n",
    "\n",
    "#   1.1. Training a decision tree regressor1 \n",
    "##==========================================\n",
    "\n",
    "treereg_1 = DecisionTreeRegressor(max_depth=2, \n",
    "                                  random_state=1)\n",
    "print(treereg_1)\n",
    "#  1.2 Fit tree to training data\n",
    "# ------------------------------\n",
    "treereg_1.fit(X_train, y_train)\n",
    "\n",
    "#         1.3. Make predictions on training set \n",
    "#.      (Yes train set, because we are still training the model)\n",
    "# -------------------------------------------------------------\n",
    "y_train_pred = treereg_1.predict(X_train)\n",
    "\n",
    "#      1.4. Compute residuals\n",
    "# ----------------------------\n",
    "errors_1 = y_train - y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(max_depth=2, random_state=1)\n"
     ]
    }
   ],
   "source": [
    "#  ============================================\n",
    "#.              Phase Two\n",
    "#  ============================================\n",
    "\n",
    "#    2.1. Initialize Decision Tree Regressor2\n",
    "# -------------------------------------------\n",
    "treereg_2 = DecisionTreeRegressor(max_depth=2, \n",
    "                                  random_state=1)\n",
    "print(treereg_2)\n",
    "#  2.2 Fit the second tree to the previous errors\n",
    "# -----------------------------------------------\n",
    "treereg_2.fit(X_train, errors_1)\n",
    "\n",
    "#  2.3. Make predictions on training set\n",
    "# --------------------------------------\n",
    "y2_train_pred = treereg_2.predict(X_train)\n",
    "\n",
    "# 2.4. Compute residuals\n",
    "# -------------------------\n",
    "errors_2 = errors_1 - y2_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  ============================================\n",
    "#.              Phase Three\n",
    "#  ============================================\n",
    "\n",
    "# 3.1 Initialize Decision Tree Regressor\n",
    "# ---------------------------------------\n",
    "treereg_3 = DecisionTreeRegressor(max_depth=2, \n",
    "                                  random_state=1)\n",
    "\n",
    "# 3.2. Fit tree to training data\n",
    "# --------------------------------\n",
    "treereg_3.fit(X_train, errors_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process would continue for a larger number of trees. But, we stop here since the principle of gradient boosting is understood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "The RMSE achieved by three tree estimators: 940.74765\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#  ============================================\n",
    "#.         Final Step: Model Evaluation\n",
    "#  ============================================\n",
    "\n",
    "# Generate predictions of each trained estimator\n",
    "# ----------------------------------------------\n",
    "first_pred = treereg_1.predict(X_test)\n",
    "\n",
    "second_pred = treereg_2.predict(X_test)\n",
    "\n",
    "third_pred = treereg_3.predict(X_test)\n",
    "\n",
    "# Aggregate all the predictions \n",
    "# ------------------------------\n",
    "final_pred = first_pred + second_pred + third_pred\n",
    "\n",
    "# Import mean_squared_error (in case in didn't do that above)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute root mean squared error (rmse)\n",
    "manual_rmse = np.sqrt(MSE(y_test, final_pred))\n",
    "print(\"-\"*70)\n",
    "print(\"The RMSE achieved by three tree estimators: {:.5f}\".format(manual_rmse))\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunatelly, there is a better way to do the previous tedious work. Thus, we will take that path to check our findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Implementing `GradientBoostingRegressor`</u> </font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we try to simulate the previous results using `GradientBoostingRegressor` estimator from `sklearn.ensemble`. Using this estimator is easier to implement and faster to run. \n",
    " \n",
    " - In order to have the same results as before, we have to use the same settings we used. This can be achieved by controlling the `GradientBoostingRegressor` hyperparameters. \n",
    "     - We used: `max_depth =2`, \n",
    "     - We need to set `n_estimators = 3` because we used 3 trees\n",
    "     - Set Random State is `random_state =1` as the same before.\n",
    "     - In the previous section, we used the errors as raw. However, in `GradientBoostingRegressor` there is a `learning_rate` hyperparameter which is set to `0.1` by default. So, we need to set it to `1` in order to have the same results. (more about learning rate later)\n",
    "     \n",
    "**Here is the list of GradientBoostingRegressor Hyperparameter**,\n",
    "```python\n",
    "========================================\n",
    "alpha                         0.9\n",
    "ccp_alpha                     0.0\n",
    "criterion                     friedman_mse\n",
    "init                          None\n",
    "learning_rate                 0.1\n",
    "loss                          squared_error\n",
    "max_depth                     3\n",
    "max_features                  None\n",
    "max_leaf_nodes                None\n",
    "min_impurity_decrease         0.0\n",
    "min_samples_leaf              1\n",
    "min_samples_split             2\n",
    "min_weight_fraction_leaf      0.0\n",
    "n_estimators                  100\n",
    "n_iter_no_change              None\n",
    "random_state                  None\n",
    "subsample                     1.0\n",
    "tol                           0.0001\n",
    "validation_fraction           0.1\n",
    "verbose                       0\n",
    "warm_start                    False\n",
    "========================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor (if you haven't done that yet)\n",
    "#-----------------------------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "The RMSE achieved by three tree estimators : 940.74765\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fitting Gradient Boosting Regressor\n",
    "# ------------------------------------\n",
    "gbreg = GradientBoostingRegressor(max_depth=2, \n",
    "                                  n_estimators=3,\n",
    "                                  random_state=1,\n",
    "                                  learning_rate=1.0)\n",
    "\n",
    "gbreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "# --------------------\n",
    "y_pred = gbreg.predict(X_test)\n",
    "\n",
    "# Compute root mean squared error (rmse)\n",
    "# -------------------------------------\n",
    "\n",
    "print(\"-\"*70)\n",
    "print((\"The RMSE achieved by three tree estimators : {:.5f}\".\n",
    "       format(np.sqrt(MSE(y_test, y_pred)))))\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see that we obtained the same results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Improving model performance by Fine-Tuning Hyperparameters</u> </font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- The purpose of this section is to get you familiar with tweaking hyperparameters. In fact, mastering this point is what distinguishes machine learning masters from novices.\n",
    "\n",
    "### The Number of Estimators  Hyperparameter `n_estimators`\n",
    "- The gradient boosting depends on the number of estimators used to transform a weak learner into a strong learner. Controlling the number of tree can be done with the hyperparameter `n_estimators`, the number of iterations. The large the number the better.\n",
    "\n",
    "- We will try different numbers of estimators to see how the algorithm performs. We choose 20, 50, 80, 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of 20 trees :  750.7028805065901\n",
      "The rmse of 50 trees :  742.2394823011787\n",
      "The rmse of 80 trees :  781.805013349976\n",
      "The rmse of 300 trees:  797.2645708630355\n"
     ]
    }
   ],
   "source": [
    "# Building Gradient boosting regressor with more 20 trees\n",
    "# ----------------------------------------------------\n",
    "\n",
    "gbr_20 = GradientBoostingRegressor(max_depth=2, \n",
    "                                n_estimators=20,\n",
    "                                random_state=1, \n",
    "                                learning_rate=1.0)\n",
    "\n",
    "gbr_20.fit(X_train, y_train) \n",
    "print(\"The rmse of 20 trees : \", np.sqrt(MSE(y_test, gbr_20.predict(X_test))))\n",
    "\n",
    "gbr_50 = GradientBoostingRegressor(max_depth=2, \n",
    "                                n_estimators=50,\n",
    "                                random_state=1, \n",
    "                                learning_rate=1.0)\n",
    "\n",
    "gbr_50.fit(X_train, y_train) \n",
    "print(\"The rmse of 50 trees : \", np.sqrt(MSE(y_test, gbr_50.predict(X_test))))\n",
    "\n",
    "gbr_80 = GradientBoostingRegressor(max_depth=2, \n",
    "                                n_estimators=80,\n",
    "                                random_state=1, \n",
    "                                learning_rate=1.0)\n",
    "\n",
    "gbr_80.fit(X_train, y_train) \n",
    "print(\"The rmse of 80 trees : \", np.sqrt(MSE(y_test, gbr_80.predict(X_test))))\n",
    "\n",
    "gbr_300 = GradientBoostingRegressor(max_depth=2, \n",
    "                                n_estimators=300,\n",
    "                                random_state=1, \n",
    "                                learning_rate=1.0)\n",
    "\n",
    "gbr_300.fit(X_train, y_train) \n",
    "print(\"The rmse of 300 trees: \", np.sqrt(MSE(y_test, gbr_300.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doing this manually certainly is not efficient, because it is tedious even with four values of trees. A better way is to write a `for loop`. (later we will see a better approach) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of 10 trees: 786.7945\n",
      "The rmse of 20 trees: 750.7029\n",
      "The rmse of 30 trees: 742.2532\n",
      "The rmse of 50 trees: 742.2395\n",
      "The rmse of 80 trees: 781.8050\n",
      "The rmse of 100 trees: 793.3260\n",
      "The rmse of 200 trees: 802.5441\n",
      "The rmse of 300 trees: 797.2646\n",
      "The rmse of 400 trees: 800.9261\n",
      "The rmse of 500 trees: 802.1516\n",
      "The rmse of 800 trees: 802.3906\n"
     ]
    }
   ],
   "source": [
    "# Trying different values for the number of estimators\n",
    "# ----------------------------------------------------\n",
    "n_estimators = [10, 20, 30, 50, 80, 100, 200, 300, 400, 500, 800]\n",
    "\n",
    "for est_num in n_estimators:\n",
    "    gbm = GradientBoostingRegressor(max_depth=2, \n",
    "                                n_estimators=est_num,\n",
    "                                random_state=1, \n",
    "                                learning_rate=1.0)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    print((\"The rmse of {} trees: {:.4f}\".format(\n",
    "        est_num, np.sqrt(MSE(y_test, gbm.predict(X_test))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait! didn't you just say that more trees would give a better performance. But we see the score improved at first then has gotten worse. $\\textbf{Why does this happen?}$ \n",
    "\n",
    "Look again at the code, what do you see?\n",
    "\n",
    "Probably you noticed (may be not) that I am using `learning_rate =1`. In other words, I am not controlling the errors (they are used fully without shrinking). Let us run the same code but with `learning_rate=0.1` (the default value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of 10 trees: 1192.3838\n",
      "The rmse of 20 trees: 932.3340\n",
      "The rmse of 30 trees: 829.5754\n",
      "The rmse of 50 trees: 744.6153\n",
      "The rmse of 80 trees: 699.4753\n",
      "The rmse of 100 trees: 685.5559\n",
      "The rmse of 200 trees: 663.8219\n",
      "The rmse of 300 trees: 640.9154\n",
      "The rmse of 400 trees: 629.6125\n",
      "The rmse of 500 trees: 630.3094\n",
      "The rmse of 800 trees: 627.0061\n",
      "The rmse of 1000 trees: 630.5315\n",
      "The rmse of 1500 trees: 633.2178\n",
      "The rmse of 1800 trees: 632.6998\n",
      "The rmse of 2000 trees: 634.3639\n",
      "The rmse of 2100 trees: 634.1741\n"
     ]
    }
   ],
   "source": [
    "# Trying different values for the number of estimators with \n",
    "# default value for learning_rate\n",
    "# ----------------------------------------------------\n",
    "n_estimators = [10, 20, 30, 50, 80, 100, 200, 300, 400, 500, 800, \n",
    "                1000, 1500, 1800, 2000, 2100]\n",
    "\n",
    "for est_num in n_estimators:\n",
    "    gbm = GradientBoostingRegressor(max_depth=2, \n",
    "                                n_estimators=est_num,\n",
    "                                random_state=1, \n",
    "                                learning_rate=0.1)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    print((\"The rmse of {} trees: {:.4f}\".format(\n",
    "        est_num, np.sqrt(MSE(y_test, gbm.predict(X_test))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Such an incredible improvement. The score has changed from 742 to 627 with 800 trees. \n",
    "\n",
    "However, there are many questions to rise here:\n",
    " - Is this best I can do?\n",
    " - Is really this model in the best? Or to what degree do we trust these results? \n",
    " \n",
    "Don't get over-excited. We are just at the begining of the journey. \n",
    "\n",
    "remember, we are using `train test split technique`. Doing cross validation might be a better choice. However, for now we keep in mind that 800 trees gave the best result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tree Depth Hyperparameter  `max_depth`\n",
    "\n",
    " - Gradient boosting is an ensemble algorithm based on a decision regressor (classifier) as a $\\textbf{base learner}$. Since you already know about decision tree regression, you should already be familiar with DTR parameters, because they are already included in the gradient boosting.  The first one we will be focusing on is `max_depth`. \n",
    " \n",
    " \n",
    "Although we found that 800 trees give the best results, I will be using only 500 trees to  reduce the time of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of 500 trees of depth None is:  733.7936 \n",
      "The rmse of 500 trees of depth 1 is:  681.8030 \n",
      "The rmse of 500 trees of depth 2 is:  630.3094 \n",
      "The rmse of 500 trees of depth 3 is:  567.3019 \n",
      "The rmse of 500 trees of depth 4 is:  589.0911 \n",
      "The rmse of 500 trees of depth 5 is:  629.7836 \n",
      "The rmse of 500 trees of depth 6 is:  607.5702 \n",
      "The rmse of 500 trees of depth 7 is:  603.0153 \n",
      "The rmse of 500 trees of depth 8 is:  661.5525 \n"
     ]
    }
   ],
   "source": [
    "# Tweaking max_depth hyperparameter\n",
    "# ---------------------------------\n",
    "depths = [None, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "for depth in depths:\n",
    "    gbr = GradientBoostingRegressor(max_depth=depth,\n",
    "                                    n_estimators=500,\n",
    "                                    random_state=1)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    print((\"The rmse of {} trees of depth {} is:  {:.4f} \".format(\n",
    "        500, depth, np.sqrt(MSE(y_test, gbr.predict(X_test))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, isn't it! We succeeded to lower the score to 567 with 500 trees and max depth of 3. \n",
    "\n",
    "We have already gained insights about the number of trees and max depth. Here, we will use `nested for loop` to find the best combination between `n_estimators` and `max_depth`. \n",
    "\n",
    "**Can we beat the score 554?** We will find out\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of 50 trees of depth None is:  733.4656 \n",
      "The rmse of 50 trees of depth 1 is:  901.1876 \n",
      "The rmse of 50 trees of depth 2 is:  744.6153 \n",
      "The rmse of 50 trees of depth 3 is:  648.1476 \n",
      "The rmse of 50 trees of depth 4 is:  641.9062 \n",
      "--------------------------------------------------\n",
      "The rmse of 100 trees of depth None is:  733.7911 \n",
      "The rmse of 100 trees of depth 1 is:  778.0273 \n",
      "The rmse of 100 trees of depth 2 is:  685.5559 \n",
      "The rmse of 100 trees of depth 3 is:  598.4261 \n",
      "The rmse of 100 trees of depth 4 is:  619.1005 \n",
      "--------------------------------------------------\n",
      "The rmse of 200 trees of depth None is:  733.7936 \n",
      "The rmse of 200 trees of depth 1 is:  714.0752 \n",
      "The rmse of 200 trees of depth 2 is:  663.8219 \n",
      "The rmse of 200 trees of depth 3 is:  582.5661 \n",
      "The rmse of 200 trees of depth 4 is:  592.9875 \n",
      "--------------------------------------------------\n",
      "The rmse of 300 trees of depth None is:  733.7936 \n",
      "The rmse of 300 trees of depth 1 is:  695.9133 \n",
      "The rmse of 300 trees of depth 2 is:  640.9154 \n",
      "The rmse of 300 trees of depth 3 is:  573.9966 \n",
      "The rmse of 300 trees of depth 4 is:  589.0556 \n",
      "--------------------------------------------------\n",
      "The rmse of 500 trees of depth None is:  733.7936 \n",
      "The rmse of 500 trees of depth 1 is:  681.8030 \n",
      "The rmse of 500 trees of depth 2 is:  630.3094 \n",
      "The rmse of 500 trees of depth 3 is:  567.3019 \n",
      "The rmse of 500 trees of depth 4 is:  589.0911 \n",
      "--------------------------------------------------\n",
      "The rmse of 600 trees of depth None is:  733.7936 \n",
      "The rmse of 600 trees of depth 1 is:  678.4918 \n",
      "The rmse of 600 trees of depth 2 is:  630.4678 \n",
      "The rmse of 600 trees of depth 3 is:  566.3488 \n",
      "The rmse of 600 trees of depth 4 is:  589.0534 \n",
      "--------------------------------------------------\n",
      "Process finished\n"
     ]
    }
   ],
   "source": [
    "# Finding the best combination between number of trees and max depth\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "n_estimators = [50, 100, 200, 300, 500, 600]\n",
    "depths = [None, 1, 2, 3, 4]\n",
    "for est_num in n_estimators:\n",
    "    for depth in depths:\n",
    "        gbr = GradientBoostingRegressor(max_depth=depth,\n",
    "                                    n_estimators=est_num,\n",
    "                                    random_state=1)\n",
    "        gbr.fit(X_train, y_train)\n",
    "        print((\"The rmse of {} trees of depth {} is:  {:.4f} \".format(\n",
    "        est_num, depth, np.sqrt(MSE(y_test, gbr.predict(X_test))))))\n",
    "    print(\"-\"*50)\n",
    "print('Process finished')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes we did! 600 trees with max depth of 3 has a lower score. but not that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "-  **learning_rate**, also known as the **shrinkage**, shrinks the contribution of individual trees so that no tree has too much influence when building the model. If an entire ensemble is built from the errors of one base learner, without careful adjustment of hyperparameters, early trees in the model can have too much influence on subsequent development. \n",
    "\n",
    "- Learning_rate limits the influence of individual trees. Generally speaking, as `n_estimators`, the number of trees, goes up, `learning_rate` should go down.\n",
    "\n",
    "- We take advantage of the results found above (max depth of 3 and 600 trees). Then we give a range of values for **learning_rate**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learning rate: 0.001, the rmse: 1367.6354\n",
      "The learning rate: 0.01, the rmse: 656.1344\n",
      "The learning rate: 0.05, the rmse: 592.3228\n",
      "The learning rate: 0.1, the rmse: 566.3488\n",
      "The learning rate: 0.15, the rmse: 600.8547\n",
      "The learning rate: 0.2, the rmse: 559.3535\n",
      "The learning rate: 0.3, the rmse: 610.0960\n",
      "The learning rate: 0.5, the rmse: 635.0904\n",
      "The learning rate: 1.0, the rmse: 885.3865\n"
     ]
    }
   ],
   "source": [
    "# Finding the best learning rate value\n",
    "# -------------------------------------\n",
    "learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]\n",
    "for lr_val in learning_rate_values:\n",
    "    gbr = GradientBoostingRegressor(max_depth=3,\n",
    "                                    n_estimators=600, \n",
    "                                    random_state=1, \n",
    "                                    learning_rate=lr_val)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    print((\"The learning rate: {}, the rmse: {:.4f}\".format(\n",
    "        lr_val, np.sqrt(MSE(y_test, gbr.predict(X_test))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We succeeded to lower the score again with learning_rate equals to `0.2` which gave us a score of `559.35`\n",
    "\n",
    "Since **learning_rate and n_estimators** are closely related, it is important to tune the together. Which we will do here using a `for loop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of 100 trees of depth 3 and learning rate value 0.1 is: 598.4261\n",
      "The rmse of 100 trees of depth 3 and learning rate value 0.15 is: 612.1888\n",
      "The rmse of 100 trees of depth 3 and learning rate value 0.2 is: 574.5496\n",
      "The rmse of 100 trees of depth 3 and learning rate value 0.3 is: 617.8604\n",
      "---------------------------------------------------------------------------\n",
      "The rmse of 300 trees of depth 3 and learning rate value 0.1 is: 573.9966\n",
      "The rmse of 300 trees of depth 3 and learning rate value 0.15 is: 603.9605\n",
      "The rmse of 300 trees of depth 3 and learning rate value 0.2 is: 561.8668\n",
      "The rmse of 300 trees of depth 3 and learning rate value 0.3 is: 608.9591\n",
      "---------------------------------------------------------------------------\n",
      "The rmse of 500 trees of depth 3 and learning rate value 0.1 is: 567.3019\n",
      "The rmse of 500 trees of depth 3 and learning rate value 0.15 is: 601.4947\n",
      "The rmse of 500 trees of depth 3 and learning rate value 0.2 is: 558.1562\n",
      "The rmse of 500 trees of depth 3 and learning rate value 0.3 is: 610.4047\n",
      "---------------------------------------------------------------------------\n",
      "The rmse of 600 trees of depth 3 and learning rate value 0.1 is: 566.3488\n",
      "The rmse of 600 trees of depth 3 and learning rate value 0.15 is: 600.8547\n",
      "The rmse of 600 trees of depth 3 and learning rate value 0.2 is: 559.3535\n",
      "The rmse of 600 trees of depth 3 and learning rate value 0.3 is: 610.0960\n",
      "---------------------------------------------------------------------------\n",
      "The rmse of 700 trees of depth 3 and learning rate value 0.1 is: 565.5323\n",
      "The rmse of 700 trees of depth 3 and learning rate value 0.15 is: 601.8704\n",
      "The rmse of 700 trees of depth 3 and learning rate value 0.2 is: 559.5456\n",
      "The rmse of 700 trees of depth 3 and learning rate value 0.3 is: 610.0169\n",
      "---------------------------------------------------------------------------\n",
      "The rmse of 1000 trees of depth 3 and learning rate value 0.1 is: 563.3032\n",
      "The rmse of 1000 trees of depth 3 and learning rate value 0.15 is: 602.5793\n",
      "The rmse of 1000 trees of depth 3 and learning rate value 0.2 is: 560.2692\n",
      "The rmse of 1000 trees of depth 3 and learning rate value 0.3 is: 610.1327\n",
      "---------------------------------------------------------------------------\n",
      "Process finished\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [100, 300, 500, 600, 700, 1000]\n",
    "learning_rate_values = [0.1, 0.15, 0.2, 0.3]\n",
    "\n",
    "for est_num in n_estimators:\n",
    "    for lr_val in learning_rate_values:\n",
    "        gbr = GradientBoostingRegressor(max_depth=3,\n",
    "                                    n_estimators=est_num, \n",
    "                                    random_state=1, \n",
    "                                    learning_rate=lr_val)\n",
    "        gbr.fit(X_train, y_train)\n",
    "        print((\"The rmse of {} trees of depth {} and learning rate value {} is: {:.4f}\".\n",
    "                   format(est_num, 3, lr_val, np.sqrt(MSE(y_test, gbr.predict(X_test))))))\n",
    "    print(\"-\"*75)\n",
    "print('Process finished') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting. 500 trees performed better than 600 trees, but the best **learning rate** is still `0.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have tried three different hyperparameters, **number of trees, max depth and learning rate**. We might be tempted to combine the three steps together using `nested for loop`, but keep in mind that this is not efficient, and we are doing it here for the purpose of teaching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of 100 trees of depth 2 and learning rate value 0.1 is: 685.5559\n",
      "The rmse of 100 trees of depth 2 and learning rate value 0.15 is: 638.1629\n",
      "The rmse of 100 trees of depth 2 and learning rate value 0.2 is: 632.6531\n",
      "===========================================================================\n",
      "The rmse of 100 trees of depth 3 and learning rate value 0.1 is: 598.4261\n",
      "The rmse of 100 trees of depth 3 and learning rate value 0.15 is: 612.1888\n",
      "The rmse of 100 trees of depth 3 and learning rate value 0.2 is: 574.5496\n",
      "===========================================================================\n",
      "The rmse of 100 trees of depth 4 and learning rate value 0.1 is: 619.1005\n",
      "The rmse of 100 trees of depth 4 and learning rate value 0.15 is: 572.2020\n",
      "The rmse of 100 trees of depth 4 and learning rate value 0.2 is: 561.8142\n",
      "===========================================================================\n",
      "---------------------------------------------------------------------------\n",
      "The rmse of 300 trees of depth 2 and learning rate value 0.1 is: 640.9154\n",
      "The rmse of 300 trees of depth 2 and learning rate value 0.15 is: 618.3628\n",
      "The rmse of 300 trees of depth 2 and learning rate value 0.2 is: 610.3102\n",
      "===========================================================================\n",
      "The rmse of 300 trees of depth 3 and learning rate value 0.1 is: 573.9966\n",
      "The rmse of 300 trees of depth 3 and learning rate value 0.15 is: 603.9605\n",
      "The rmse of 300 trees of depth 3 and learning rate value 0.2 is: 561.8668\n",
      "===========================================================================\n",
      "The rmse of 300 trees of depth 4 and learning rate value 0.1 is: 589.0556\n",
      "The rmse of 300 trees of depth 4 and learning rate value 0.15 is: 561.3104\n",
      "The rmse of 300 trees of depth 4 and learning rate value 0.2 is: 555.2921\n",
      "===========================================================================\n",
      "---------------------------------------------------------------------------\n",
      "The rmse of 500 trees of depth 2 and learning rate value 0.1 is: 630.3094\n",
      "The rmse of 500 trees of depth 2 and learning rate value 0.15 is: 619.0584\n",
      "The rmse of 500 trees of depth 2 and learning rate value 0.2 is: 607.5764\n",
      "===========================================================================\n",
      "The rmse of 500 trees of depth 3 and learning rate value 0.1 is: 567.3019\n",
      "The rmse of 500 trees of depth 3 and learning rate value 0.15 is: 601.4947\n",
      "The rmse of 500 trees of depth 3 and learning rate value 0.2 is: 558.1562\n",
      "===========================================================================\n",
      "The rmse of 500 trees of depth 4 and learning rate value 0.1 is: 589.0911\n",
      "The rmse of 500 trees of depth 4 and learning rate value 0.15 is: 560.6493\n",
      "The rmse of 500 trees of depth 4 and learning rate value 0.2 is: 556.0730\n",
      "===========================================================================\n",
      "---------------------------------------------------------------------------\n",
      "The rmse of 600 trees of depth 2 and learning rate value 0.1 is: 630.4678\n",
      "The rmse of 600 trees of depth 2 and learning rate value 0.15 is: 621.8417\n",
      "The rmse of 600 trees of depth 2 and learning rate value 0.2 is: 608.4789\n",
      "===========================================================================\n",
      "The rmse of 600 trees of depth 3 and learning rate value 0.1 is: 566.3488\n",
      "The rmse of 600 trees of depth 3 and learning rate value 0.15 is: 600.8547\n",
      "The rmse of 600 trees of depth 3 and learning rate value 0.2 is: 559.3535\n",
      "===========================================================================\n",
      "The rmse of 600 trees of depth 4 and learning rate value 0.1 is: 589.0534\n",
      "The rmse of 600 trees of depth 4 and learning rate value 0.15 is: 560.1450\n",
      "The rmse of 600 trees of depth 4 and learning rate value 0.2 is: 556.1285\n",
      "===========================================================================\n",
      "---------------------------------------------------------------------------\n",
      "Process finished\n"
     ]
    }
   ],
   "source": [
    "# Fine Tuning three hyperparameters at once\n",
    "# -----------------------------------------\n",
    "\n",
    "n_estimators = [100, 300, 500, 600]\n",
    "depths = [2, 3, 4]\n",
    "learning_rate_values = [0.1, 0.15, 0.2]\n",
    "for est_num in n_estimators:\n",
    "    for depth in depths:\n",
    "        for lr_val in learning_rate_values:\n",
    "            gbr = GradientBoostingRegressor(max_depth=depth,\n",
    "                                    n_estimators=est_num, \n",
    "                                    random_state=1, \n",
    "                                    learning_rate=lr_val)\n",
    "            gbr.fit(X_train, y_train)\n",
    "            print((\"The rmse of {} trees of depth {} and learning rate value {} is: {:.4f}\".\n",
    "                   format(est_num, depth, lr_val, np.sqrt(MSE(y_test, gbr.predict(X_test))))))\n",
    "        print(\"=\"*75)\n",
    "    print(\"-\"*75)\n",
    "print('Process finished') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we see that `max_depth` of 4 with `learning_rate` of `0.2` and 300 trees is slightly better (`555.2921`) than the results found above (`558.1562`)\n",
    "\n",
    "This is one of the methods of hyperparameter tuning. There are other approaches such as seach for a bunch of them at once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample\n",
    "\n",
    "- **Subsample** is a subset of samples. Since samples are the rows, a subset of rows means that all rows may not be included when building each tree. By changing subsample from 1.0 to a smaller decimal, trees only select that percentage of samples during the build phase. For example, subsample=0.8 would select 80% of samples for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsample: 1 , Score: 555.2921208406199\n",
      "Subsample: 0.9 , Score: 585.6108529421044\n",
      "Subsample: 0.8 , Score: 609.2729801702583\n",
      "Subsample: 0.7 , Score: 599.9426599654848\n",
      "Subsample: 0.6 , Score: 594.3070982484685\n",
      "Subsample: 0.5 , Score: 654.7627510998128\n",
      "Subsample: 0.4 , Score: 645.7461686872832\n"
     ]
    }
   ],
   "source": [
    "samples = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4]\n",
    "for sample in samples:\n",
    "    gbr = GradientBoostingRegressor(max_depth=4,\n",
    "                                    n_estimators=300,\n",
    "                                    subsample=sample,\n",
    "                                    learning_rate=0.2,\n",
    "                                    random_state=1)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Subsample:', sample, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sabsample** did not improve the results, and it suggest using the entire training data for training each estimator.\n",
    "\n",
    "**Can we improve the model even more?**  \n",
    "\n",
    "Using cross-validation to confirm these findings would be a great choice. However, I am not going to doing manually. \n",
    "\n",
    "Now, I will turn into more practical ways of hyperparameter fine tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Automating Hyperparameter Fine-Tuning</u> </font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning Algorithms \n",
    "\n",
    "- There are different approaches for fine tuning hyperparameters\n",
    "    1. **Grid Search CV**: it is an exhaustive algorithm that searches all possible combinations in a hyperparameter grid to find the best results.\n",
    "    2. **Randomized Search CV**: it a random searching algorithm that selects random hyperparameter combinations (10 by default) to search through. **RandomizedSearchCV** is typically used when GridSearchCV becomes unwieldy because there are too many hyperparameter combinations to exhaustively check each one.\n",
    "    3. **Bayesian Optimization**: This is an informative searching algorithm. Bayesian hyperparameter tuning is popular for larger and more complex hyperparameter tuning tasks. In other words, It can optimize a model with hundreds of parameters on a large scale. (**The python package that serves this purpose is called \"hyperopt\")\n",
    "    4. **Genetic algorithm**:  It is another informed search methodology using genetic hyperparameter tuning. It applies the principle of biological evolution into machine learning. (We won't go into the details here, but if you are into genetic programming you can use \"TPOT\" python package to learn about this powerful technique) \n",
    "    \n",
    "    \n",
    "**You might be insterested in other hyperparameter tuning algorithms, you can check these libraries**:\n",
    " 1. `scikit optimize` (see the [documentatio](https://pypi.org/project/scikit-optimize/)) and [here](https://scikit-optimize.github.io/stable/) as well.\n",
    " 2. `optuna` (see the [documentatio](https://pypi.org/project/optuna/))\n",
    " \n",
    " 3. `sigopt`: this is highly recommended as you going to use **XGBoost** in your projects, you can check [here](https://app.sigopt.com/docs/intro/overview) or visti their github page [here](https://github.com/sigopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Grid Search Cross Validation</u> </font>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.05, 'n_estimators': 600, 'subsample': 0.5}\n",
      "Training score: 617.025\n",
      "Test set score: 610.879\n"
     ]
    }
   ],
   "source": [
    "params={'subsample':[0.5, 0.7, 0.9, 1],\n",
    "        'n_estimators':[300, 500, 600, 1000],\n",
    "        'learning_rate':[0.05, 0.075, 0.1, 0.15, 0.2, 0.3]}\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbreg = GradientBoostingRegressor(max_depth=3, \n",
    "                                random_state=1)\n",
    "\n",
    "\n",
    "# Instantiate RandomizedSearchCV as gridcv_reg\n",
    "# ------------------------------------------\n",
    "gridcv_reg = GridSearchCV(estimator=gbreg,\n",
    "                              param_grid=params,\n",
    "                              scoring='neg_mean_squared_error', \n",
    "                              cv=5, \n",
    "                              n_jobs=-1)\n",
    "\n",
    "# Fit grid_reg on X_train and y_train\n",
    "# -----------------------------------\n",
    "gridcv_reg.fit(X_train, y_train)\n",
    "\n",
    "# Extract best estimator\n",
    "# -----------------------\n",
    "best_model = gridcv_reg.best_estimator_\n",
    "\n",
    "# Extract best params\n",
    "# -------------------\n",
    "best_params = gridcv_reg.best_params_\n",
    "\n",
    "# Print best params\n",
    "# -----------------\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Compute best score\n",
    "# ------------------\n",
    "best_score = np.sqrt(np.abs(gridcv_reg.best_score_))\n",
    "\n",
    "# Print best score\n",
    "# -----------------\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "# Predict test set labels\n",
    "# -----------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "# -----------------\n",
    "rmse_test = MSE(y_test, y_pred)**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "# ---------------\n",
    "print('Test set score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Randomized Search Cross Validation</u> </font>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.5, 'n_estimators': 1000, 'learning_rate': 0.02}\n",
      "Training score: 613.889\n",
      "Test set score: 589.547\n"
     ]
    }
   ],
   "source": [
    "# First try of finding the best hyperparameters\n",
    "# ---------------------------------------------\n",
    "params={'subsample':[0.5, 0.6, 0.7, 0.75, 0.9, 1],\n",
    "        'n_estimators':[300, 500, 600, 800, 1000],\n",
    "        'learning_rate': [0.01, 0.02, 0.1, 0.2, 0.3]}\n",
    "\n",
    "# Import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbreg = GradientBoostingRegressor(max_depth=3, \n",
    "                                random_state=1)\n",
    "\n",
    "\n",
    "# Instantiate RandomizedSearchCV to search through 50 hyperparameters\n",
    "# -------------------------------------------------------------------\n",
    "randcv_reg = RandomizedSearchCV(estimator=gbreg,\n",
    "                              param_distributions=params,\n",
    "                              n_iter=50,\n",
    "                              scoring='neg_mean_squared_error', \n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              random_state=1)\n",
    "\n",
    "# Fit grid_reg on X_train and y_train\n",
    "# -----------------------------------\n",
    "randcv_reg.fit(X_train, y_train)\n",
    "\n",
    "# Extract best estimator\n",
    "# -----------------------\n",
    "best_model = randcv_reg.best_estimator_\n",
    "\n",
    "# Extract best params\n",
    "# -------------------\n",
    "best_params = randcv_reg.best_params_\n",
    "\n",
    "# Print best params\n",
    "# -----------------\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Compute best score\n",
    "# ------------------\n",
    "best_score = np.sqrt(np.abs(randcv_reg.best_score_))\n",
    "\n",
    "# Print best score\n",
    "# -----------------\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "# Predict test set labels\n",
    "# -----------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "# -----------------\n",
    "rmse_test = np.sqrt(MSE(y_test, y_pred))\n",
    "\n",
    "# Print rmse_test\n",
    "# ---------------\n",
    "print('Test set score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to expand the range hyperparameters in hope to get better performance. \n",
    "\n",
    "Since the found number of n_estimators is 1000, I will add more trees. The same goes learning rate and subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.7999999999999999, 'n_estimators': 1600, 'learning_rate': 0.101}\n",
      "Training score: 639.061\n",
      "Test set score: 570.466\n"
     ]
    }
   ],
   "source": [
    "params={'subsample':np.arange(0.5, 1, 0.1),\n",
    "        'n_estimators':[1500, 1600, 1700, 1800, 2000],\n",
    "        'learning_rate':np.arange(0.001, 1, 0.1)}\n",
    "\n",
    "# Import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbreg = GradientBoostingRegressor(max_depth=3, \n",
    "                                random_state=1)\n",
    "\n",
    "\n",
    "# Instantiate RandomizedSearchCV as rand_reg\n",
    "# ------------------------------------------\n",
    "randcv_reg = RandomizedSearchCV(estimator=gbreg,\n",
    "                              param_distributions=params,\n",
    "                              n_iter=50,\n",
    "                              scoring='neg_mean_squared_error', \n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              random_state=1)\n",
    "\n",
    "# Fit grid_reg on X_train and y_train\n",
    "# -----------------------------------\n",
    "randcv_reg.fit(X_train, y_train)\n",
    "\n",
    "# Extract best estimator\n",
    "# -----------------------\n",
    "best_model = randcv_reg.best_estimator_\n",
    "\n",
    "# Extract best params\n",
    "# -------------------\n",
    "best_params = randcv_reg.best_params_\n",
    "\n",
    "# Print best params\n",
    "# -----------------\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Compute best score\n",
    "# ------------------\n",
    "best_score = np.sqrt(np.abs(randcv_reg.best_score_))\n",
    "\n",
    "# Print best score\n",
    "# -----------------\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "# Predict test set labels\n",
    "# -----------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "# -----------------\n",
    "rmse_test = MSE(y_test, y_pred)**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "# ---------------\n",
    "print('Test set score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# the best found results\n",
    "#-----------------------\n",
    "Best params: {'subsample': 0.7999999999999999, \n",
    "            'n_estimators': 1600, \n",
    "            'learning_rate': 0.101}\n",
    "            \n",
    "Training score: 639.061\n",
    "Test set score: 570.466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "The best rmse score achieved is 565.54146\n",
      "****************************************\n",
      "\n",
      "Run Time: 1.1967270374298096 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Fitting the best tuned model\n",
    "# -----------------------------\n",
    "\n",
    "#------------------\n",
    "# Start run time\n",
    "\n",
    "start = time.time()\n",
    "# ---------------\n",
    "\n",
    "final_model = GradientBoostingRegressor(max_depth=3,\n",
    "                                n_estimators=1600, \n",
    "                                subsample=0.799, \n",
    "                                learning_rate=0.101,\n",
    "                                random_state=1)\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "print(\"*\"*40)\n",
    "print(\"The best rmse score achieved is {:0.5f}\".format(np.sqrt(MSE(y_test, y_pred))))\n",
    "print(\"*\"*40)\n",
    "\n",
    "#------------------\n",
    "# End run time\n",
    "\n",
    "end = time.time()\n",
    "# ---------------\n",
    "\n",
    "elapsed = end - start\n",
    "print('\\nRun Time: ' + str(elapsed) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Testing the model using Cross Validation</u> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse:  [ 502.775  456.976  452.186  474.279  475.225  430.983  761.128  604.692\n",
      "  663.948  933.82   779.394  750.842  753.047  683.094  703.178  709.154\n",
      "  880.231  639.928 1171.567 2042.703]\n",
      "RMSE mean: 743.457\n"
     ]
    }
   ],
   "source": [
    "# Obtain scores of cross-validation using 10 splits and mean squared error\n",
    "# ========================================================================\n",
    "\n",
    "scores = cross_val_score(final_model,\n",
    "                         X,\n",
    "                         y,\n",
    "                         scoring='neg_mean_squared_error', \n",
    "                         cv=20)\n",
    "\n",
    "# Take square root of the scores\n",
    "# ------------------------------\n",
    "rmse = np.sqrt(np.abs(scores))\n",
    "\n",
    "# Display root mean squared error\n",
    "# -------------------------------\n",
    "print('Reg rmse: ', np.round(rmse, 3))\n",
    "\n",
    "# Display mean score\n",
    "# ------------------\n",
    "print('RMSE mean: {:0.3f}'.format(rmse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whaaaat?** why the score has gone up? \n",
    "\n",
    "The answer might obvious to you. This happened because, the first model is tested only on one fold (tested only once), while when using cross validation the model is tested on different folds. \n",
    "\n",
    "What does this tell you? The score is not necessarily the most important thing. Therefore, don't get too excited because you get a good score. But, you need to test your model in a consistent way. \n",
    "\n",
    "Finally, I will give a last check using repeated cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse:  [488.93  573.523 567.371 602.61  632.155 545.671 678.988 631.918 593.892\n",
      " 693.886 512.759 568.379 872.063 601.529 581.491 466.794 621.578 475.546\n",
      " 770.999 765.85  582.126 524.464 743.407 608.302 708.318 615.753 470.132\n",
      " 710.137 661.876 665.605]\n",
      "RMSE mean: 617.868\n",
      "\n",
      "Run Time: 39.36495304107666 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Import repeatedKFold\n",
    "# --------------------\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "#------------------\n",
    "start = time.time()\n",
    "#------------------\n",
    "\n",
    "# Intantiate the Repeated CV\n",
    "#----------------------------\n",
    "cv = RepeatedKFold(n_splits=10, \n",
    "                   n_repeats=3, \n",
    "                   random_state=1)\n",
    "\n",
    "scores = cross_val_score(final_model,\n",
    "                         X,\n",
    "                         y,\n",
    "                         scoring='neg_mean_squared_error', \n",
    "                         cv=cv)\n",
    "\n",
    "# Take square root of the scores\n",
    "# ------------------------------\n",
    "rmse = np.sqrt(np.abs(scores))\n",
    "\n",
    "# Display root mean squared error\n",
    "# -------------------------------\n",
    "print('Reg rmse: ', np.round(rmse, 3))\n",
    "\n",
    "# Display mean score\n",
    "# ------------------\n",
    "print('RMSE mean: {:0.3f}'.format(rmse.mean()))\n",
    "#------------------\n",
    "end = time.time()\n",
    "#------------------\n",
    "\n",
    "elapsed = end - start\n",
    "print('\\nRun Time: ' + str(elapsed) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are doing a good job anyway.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Bayesian Optimization with hyperopt</u> </font> \n",
    "    \n",
    "In this section, you need to install `hyperopt` package. This the [documentation](http://hyperopt.github.io/hyperopt/) for more information about this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================================================\n",
    "#      Hyperparameter tuning with Bayesian Optimization  \n",
    "## ======================================================\n",
    "\n",
    "# Import the tools from hyperopt\n",
    "# ------------------------------\n",
    "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
    "\n",
    "# Set up space dictionary with specified hyperparameters\n",
    "# -----------------------------------------------------\n",
    "space = {\"max_depth\": hp.quniform(\"max_depth\", 1, 6,1),\n",
    "         'subsample': hp.quniform('subsample', 0.5, 1, 0.1),\n",
    "         'n_estimators': hp.quniform(\"n_estimators\", 1500, 2000, 100),\n",
    "         'learning_rate': hp.quniform('learning_rate', 0.001, 1, 0.1)}\n",
    "\n",
    "\n",
    "# Set up objective function\n",
    "# ------------------------\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),\n",
    "              'subsample': params['subsample'],\n",
    "              'n_estimators': int(params['n_estimators']),\n",
    "              'learning_rate': params['learning_rate'],}\n",
    "    \n",
    "    gbm_reg = GradientBoostingRegressor(**params) \n",
    "    scores = cross_val_score(gbm_reg, \n",
    "                             X_train, y_train, \n",
    "                             scoring='neg_mean_squared_error', \n",
    "                             cv=5,\n",
    "                             n_jobs=8)\n",
    "    best_score = np.sqrt(np.abs(scores).mean())\n",
    "    return {'loss': best_score, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:09<00:00,  1.09trial/s, best loss: 663.6454421634694]\n",
      "{'learning_rate': 0.1, 'max_depth': 5.0, 'n_estimators': 1800.0, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn= objective,\n",
    "            space=space, \n",
    "            max_evals= 10, \n",
    "            algo=tpe.suggest,\n",
    "            trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The results of 500 iterations\n",
    "# ------------------------------\n",
    "100%|| 1500/1500 [03:25<00:00,  7.28trial/s, best loss: 630.7461476369226]\n",
    "{'learning_rate': 0.1, 'max_depth': 3.0, 'n_estimators': 1800.0, 'subsample': 0.6000000000000001}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Genetic Hyperparameter Fine Tuning</u> </font> \n",
    "    \n",
    "In this section, you need to install `tpot` package. This the [documentation](http://epistasislab.github.io/tpot/) for more information about this library.\n",
    "    \n",
    "Note that in real life, `TPOT` is designed to be run for many hours to find the best model.\n",
    "    \n",
    "In order to have good results:\n",
    "    \n",
    "    1. Use a larger number of population\n",
    "    \n",
    "    2. A larger size offspring size\n",
    "    \n",
    "    3. Hundreds or thousands of generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pipeline: XGBRegressor(input_matrix, learning_rate=0.1, max_depth=8, min_child_weight=12, n_estimators=100, n_jobs=1, objective=reg:squarederror, subsample=0.8, verbosity=0)\n",
      "\n",
      "\n",
      "604.9633203883452\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ======================================================\n",
    "#      Hyperparameter tuning with Bayesian Optimization  \n",
    "## ======================================================\n",
    "\n",
    "# Import the regressor\n",
    "# ---------------------\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "\n",
    "# Create the tpot regressor\n",
    "#-------------------------\n",
    "tpot_reg = TPOTRegressor(generations= 3,\n",
    "                          population_size= 10,\n",
    "                          offspring_size= 3, \n",
    "                          scoring= 'neg_mean_squared_error',\n",
    "                          verbosity=1,\n",
    "                          random_state=2, \n",
    "                          cv=5)\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "# --------------------------------------\n",
    "tpot_reg.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "# ------------------------\n",
    "print(\"\\n\")\n",
    "print(np.sqrt(np.abs(tpot_reg.score(X_test, y_test))))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Try these settings in your own time\n",
    "#number_generations = 100\n",
    "#population_size = 10\n",
    "#offspring_size = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The findings are outlined here\n",
    "# -----------------------------\n",
    "Best pipeline: XGBRegressor(LinearSVR(ZeroCount(input_matrix), C=5.0, dual=True, epsilon=0.0001, loss=squared_epsilon_insensitive, tol=1e-05), learning_rate=0.1, max_depth=8, min_child_weight=12, n_estimators=100, n_jobs=1, objective=reg:squarederror, subsample=0.8, verbosity=0)\n",
    "567.5762794891554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Implementing XGBoost </u> </font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In order to be able apply this section, you need to have `XGBoost` python library installed on your system. If you haven't done that before,  please do now by using this syntax:\n",
    " \n",
    "```python \n",
    "pip install xgboost \n",
    "or(!pip install if you are using jupyter notebook)\n",
    "or \n",
    "conda install xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "The XGBoost Score is: 563.64302\n",
      "****************************************\n",
      "\n",
      "Run Time: 0.6026620864868164 seconds.\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "#             Training XGBoost Regressor\n",
    "# =================================================================\n",
    "\n",
    "# Import XGBRegressor\n",
    "# --------------------\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Instantiate the XGBRegressor using the same results found\n",
    "# when training GBM model\n",
    "# ----------------------------------------------------------\n",
    "import time\n",
    "#------------------\n",
    "# Start run time\n",
    "\n",
    "start = time.time()\n",
    "# ---------------\n",
    "\n",
    "\n",
    "xgb_reg = XGBRegressor(max_depth=3,\n",
    "                      n_estimators=1600,\n",
    "                      eta=0.02,             # This is the learning rate\n",
    "                      subsample=0.799,\n",
    "                      random_state=1)\n",
    "\n",
    "# Fit the regressor to training set\n",
    "# ------------------------------\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction on test set\n",
    "# ---------------------------\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "#------------------\n",
    "# End run time\n",
    "\n",
    "end = time.time()\n",
    "# ---------------\n",
    "\n",
    "\n",
    "# Compute root mean squared error (rmse) and print the result\n",
    "# -----------------------------------------------------------\n",
    "print(\"*\"*40)\n",
    "print(\"The XGBoost Score is: {:.5f}\".format(np.sqrt(MSE(y_test, y_pred))))\n",
    "print(\"*\"*40)\n",
    "\n",
    "elapsed = end - start\n",
    "print('\\nRun Time: ' + str(elapsed) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite impressive that XGBoost beated Gradient boosting hands down. Also, notice the speed. The model ran faster than GBM. \n",
    "\n",
    "It's another reason that motivates you to pay more attention to this highly sophisticated algorithm (and a package as well).\n",
    "\n",
    "**What are the questions running in your mind?**\n",
    "\n",
    "  - Should we do the rally of tuning xgboost hyperparameter? the answer is yes\n",
    "  - Should we fully rely oh this result? the answer is no. Because we tested our model only on a subset of data. Using cross validation would be the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>XGBoost model using Cross Validation</u> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [488.93  573.523 567.371 602.61  632.155 545.671 678.988 631.918 593.892\n",
      " 693.886 512.759 568.379 872.063 601.529 581.491 466.794 621.578 475.546\n",
      " 770.999 765.85  582.126 524.464 743.407 608.302 708.318 615.753 470.132\n",
      " 710.137 661.876 665.605]\n",
      "RMSE mean: 617.868\n",
      "\n",
      "Run Time: 0.862098217010498 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Instantiate XGBRegressor\n",
    "# ------------------------\n",
    "xgb_model = XGBRegressor(objective=\"reg:squarederror\")\n",
    "\n",
    "# Obtain scores of cross-validation using 10 splits and mean squared error\n",
    "xgb_scores = cross_val_score(xgb_model,\n",
    "                         X,\n",
    "                         y,\n",
    "                         scoring='neg_mean_squared_error', \n",
    "                         cv=10)\n",
    "\n",
    "# Take square root of the xgb_scores\n",
    "# ----------------------------------\n",
    "xgb_rmse = np.sqrt(np.abs(scores))\n",
    "\n",
    "# Display root mean squared error\n",
    "# --------------------------------\n",
    "print('Reg rmse:', np.round(xgb_rmse, 3))\n",
    "\n",
    "# Display mean score\n",
    "# ------------------\n",
    "print('RMSE mean: {:0.3f}'.format(xgb_rmse.mean()))\n",
    "\n",
    "#------------------\n",
    "end = time.time()\n",
    "#------------------\n",
    "\n",
    "elapsed = end - start\n",
    "print('\\nRun Time: ' + str(elapsed) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results isn't good. Why don't try repeated cross validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse:  [554.134 697.054 677.265 636.77  625.903 632.447 739.251 701.855 673.566\n",
      " 766.237 606.343 603.48  983.432 581.828 662.286 610.144 677.445 632.654\n",
      " 800.916 817.209 615.848 617.257 902.243 741.963 710.875 615.803 499.381\n",
      " 751.938 556.028 733.715]\n",
      "RMSE mean: 680.842\n",
      "\n",
      "Run Time: 2.61869740486145 seconds.\n"
     ]
    }
   ],
   "source": [
    "#------------------\n",
    "start = time.time()\n",
    "#------------------\n",
    "\n",
    "xgb_model = XGBRegressor(objective=\"reg:squarederror\")\n",
    "\n",
    "# Intantiate the Repeated CV\n",
    "#----------------------------\n",
    "cv = RepeatedKFold(n_splits=10, \n",
    "                   n_repeats=3, \n",
    "                   random_state=1)\n",
    "\n",
    "xgb_scores = cross_val_score(xgb_model,\n",
    "                         X,\n",
    "                         y,\n",
    "                         scoring='neg_mean_squared_error', \n",
    "                         cv=cv)\n",
    "\n",
    "# Take square root of the scores\n",
    "# ------------------------------\n",
    "rmse = np.sqrt(np.abs(xgb_scores))\n",
    "\n",
    "# Display root mean squared error\n",
    "# -------------------------------\n",
    "print('Reg rmse: ', np.round(rmse, 3))\n",
    "\n",
    "# Display mean score\n",
    "# ------------------\n",
    "print('RMSE mean: {:0.3f}'.format(rmse.mean()))\n",
    "#------------------\n",
    "end = time.time()\n",
    "#------------------\n",
    "\n",
    "elapsed = end - start\n",
    "print('\\nRun Time: ' + str(elapsed) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning for xgboost model is necessary**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I hope this tutorial widens your horizon and opens new doors to you in your journey of machine learning field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
